{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import neuralNets\n",
    "import time\n",
    "\n",
    "def oneHotEncodeData3Classes(targets):\n",
    "    j=0\n",
    "    Y_val = np.zeros((targets.shape[0], 3))\n",
    "    for j in range(targets.shape[0]):\n",
    "        if targets[j] == 0:\n",
    "            Y_val[j, 0] = 1\n",
    "        elif targets[j] == 1:\n",
    "            Y_val[j, 1] = 1\n",
    "        elif targets[j] == 2:\n",
    "            Y_val[j, 2] = 1\n",
    "        else:\n",
    "            print(\"something went wrong, new class\", targets[j])\n",
    "    return Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(\"data/numerai_datasets_04.04.21/numerai_validation_data.csv\")  \n",
    "feature_cols = validation_data.columns[validation_data.columns.str.startswith('feature')]\n",
    "X_val = validation_data[feature_cols].to_numpy()\n",
    "Y_val = validation_data.target.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine between the 3 class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_3classes = neuralNets.defineNN_3classes(X_val.shape[1])\n",
    "modelNN_3classes.compile(optimizer='adam', loss='categorical_crossentropy', metrics='categorical_accuracy')\n",
    "modelNN_3classes.load_weights(\"model_3class_99train_99val_4_4_21.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_3class = modelNN_3classes.predict(X_val)\n",
    "pred_3class = np.argmax(pred_3class, axis = 1) #convert one hot vectors to labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_01 = neuralNets.defineNN_2classes(X_val.shape[1])\n",
    "modelNN_01.load_weights(\"model_01class_99train_99test_994val.h5\")\n",
    "\n",
    "pred_01 = np.rint(modelNN_01.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# determine between 0.25 and 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_025075 = neuralNets.defineNN_2classes(X_val.shape[1])\n",
    "modelNN_025075.load_weights(\"model_025075_class_998train__998val.h5\")\n",
    "\n",
    "pred_025075 = np.rint(modelNN_025075.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge results to overall prediction and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_targets  = pred_3class.shape[0]\n",
    "prediction = np.zeros((n_targets, 1))\n",
    "\n",
    "for i in range(n_targets):\n",
    "    if pred_3class[i] == 0:                     # label either 0 or 1\n",
    "        prediction[i] = pred_01[i]\n",
    "    elif pred_3class[i] == 1:                # label either 0.25 or 0.75\n",
    "        if pred_025075[i] == 0:\n",
    "            prediction[i] = 0.25\n",
    "        else:\n",
    "            assert(pred_025075[i] == 1)\n",
    "            prediction[i] = 0.75\n",
    "    else:                                       # label is 0.5\n",
    "        assert(pred_3class[i] == 2)\n",
    "        prediction[i] = 0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correctPred = 0\n",
    "\n",
    "n_025075_confusion = 0\n",
    "n_01_confusion     = 0\n",
    "n_3class_confusin  = 0\n",
    "\n",
    "n_0   = validation_data.target.loc[validation_data.target == 0].shape[0]\n",
    "n_025 = validation_data.target.loc[validation_data.target == 0.25].shape[0]\n",
    "n_05  = validation_data.target.loc[validation_data.target == 0.5].shape[0]\n",
    "n_075 = validation_data.target.loc[validation_data.target == 0.75].shape[0]\n",
    "n_1   = validation_data.target.loc[validation_data.target == 1].shape[0]\n",
    "\n",
    "indexArray = []\n",
    "\n",
    "for i in range(n_targets):\n",
    "    if prediction[i] == Y_val[i]:\n",
    "        n_correctPred += 1\n",
    "    else:\n",
    "        is025075confusion = (prediction[i] == 0.25 and Y_val[i] == 0.75) or (prediction[i] == 0.75 and Y_val[i] == 0.25)\n",
    "        is01confusion     = (prediction[i] == 1 and Y_val[i] == 0) or (prediction[i] == 1 and Y_val[i] == 0)\n",
    "        \n",
    "        if is025075confusion:\n",
    "            n_025075_confusion += 1\n",
    "            indexArray.append(i)\n",
    "        elif is01confusion:\n",
    "            n_01_confusion += 1\n",
    "        else:\n",
    "            n_3class_confusin += 1\n",
    "        \n",
    "accuracy = n_correctPred/n_targets\n",
    "\n",
    "print(\"accuracy \", accuracy, \"                   total number of examples =\", n_targets)\n",
    "print(\"0.25 vs 0.75 confusion nunber = \", n_025075_confusion, \"    vs. total number \", n_025 + n_075, \"in per \", n_025075_confusion/(n_025+n_075))\n",
    "print(\"0    vs 1    confusion number = \", n_01_confusion, \"       vs. total number \", n_0 + n_1, \"in per \", n_01_confusion/(n_0+n_1))\n",
    "print(\"3class confusion = \", n_3class_confusin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_pred = validation_data.copy()\n",
    "validation_data_pred[\"prediction\"] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "PREDICTION_NAME = \"prediction\"\n",
    "def feature_exposures(df):\n",
    "    feature_names = [f for f in df.columns\n",
    "                     if f.startswith(\"feature\")]\n",
    "    #print(feature_names)\n",
    "    exposures = []\n",
    "    for f in feature_names:\n",
    "        fe = spearmanr(df[PREDICTION_NAME], df[f])[0]\n",
    "        #print(fe)\n",
    "        exposures.append(fe)\n",
    "    return np.array(exposures)\n",
    "\n",
    "\n",
    "def max_feature_exposure(df):\n",
    "    return np.max(np.abs(feature_exposures(df)))\n",
    "\n",
    "def max_feature_exposure(featureExposure):\n",
    "    return np.max(np.abs(featureExposure))\n",
    "\n",
    "\n",
    "def feature_exposure(df):\n",
    "    return np.sqrt(np.mean(np.square(feature_exposures(df))))\n",
    "\n",
    "def feature_exposure(featureExposure):\n",
    "    return np.sqrt(np.mean(np.square(featureExposure)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExposures = feature_exposures(validation_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxExposure = max_feature_exposure(featureExposures)\n",
    "featureExposureArray = feature_exposure(featureExposures)\n",
    "\n",
    "print(maxExposure)\n",
    "print(featureExposureArray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
