{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "\n",
    "import neuralNets\n",
    "\n",
    "def oneHotEncodeData3Classes(targets):\n",
    "    j=0\n",
    "    Y_val = np.zeros((targets.shape[0], 3))\n",
    "    for j in range(targets.shape[0]):\n",
    "        if targets[j] == 0:\n",
    "            Y_val[j, 0] = 1\n",
    "        elif targets[j] == 1:\n",
    "            Y_val[j, 1] = 1\n",
    "        elif targets[j] == 2:\n",
    "            Y_val[j, 2] = 1\n",
    "        else:\n",
    "            print(\"something went wrong, new class\", targets[j])\n",
    "    return Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "#training_data = pd.read_csv(\"appendedTrainingData_test12021-01-31_2021-03-21.csv\")\n",
    "training_data = pd.read_csv(\"data/numerai_datasets_04.04.21/numerai_training_data.csv\")\n",
    "feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n",
    "\n",
    "training_data[feature_cols] = training_data[feature_cols].astype(np.float16)\n",
    "training_data.target        = training_data.target.astype(np.float16)\n",
    "\n",
    "# X_train = training_data[feature_cols].to_numpy()\n",
    "# Y_train = training_data.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_data = pd.read_csv(\"data/numerai_datasets_28.03.21/numerai_tournament_data.csv\")  \n",
    "tournament_data[feature_cols] = tournament_data[feature_cols].astype(np.float16)\n",
    "tournament_data.target        = tournament_data.target.astype(np.float16)\n",
    "\n",
    "validation_data = tournament_data.loc[tournament_data.data_type == 'validation']\n",
    "X_val = validation_data[feature_cols].reset_index().drop(['index'], axis = 1).to_numpy()\n",
    "#X_pred = tournamend_data.loc[tournament_data.data_type == \"\"]\n",
    "Y_val = validation_data.target.to_numpy()\n",
    "\n",
    "X_val_3class = training_data[feature_cols]\n",
    "Y_val_3class = training_data.target\n",
    "\n",
    "Y_val_3class = Y_val_3class.replace(1, 0)\n",
    "Y_val_3class = Y_val_3class.replace([0.25, 0.75], 1)\n",
    "Y_val_3class = Y_val_3class.replace(0.5, 2)\n",
    "\n",
    "X_val_3class = X_val_3class.to_numpy()\n",
    "Y_val_3class = Y_val_3class.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(\"data/numerai_datasets_04.04.21/numerai_validation_data.csv\")  \n",
    "\n",
    "X_val_3class = validation_data[feature_cols]\n",
    "Y_val_3class = validation_data.target\n",
    "\n",
    "Y_val_3class = Y_val_3class.replace(1, 0)\n",
    "Y_val_3class = Y_val_3class.replace([0.25, 0.75], 1)\n",
    "Y_val_3class = Y_val_3class.replace(0.5, 2)\n",
    "\n",
    "X_val_3class = X_val_3class.to_numpy()\n",
    "Y_val_3class = Y_val_3class.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tournament_data.to_csv(\"data/numerai_datasets_28.03.21/numerai_tournament_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier between 0/1 0.25/0.75 and 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3class = training_data[feature_cols]\n",
    "Y_train_3class = training_data.target\n",
    "\n",
    "Y_train_3class = Y_train_3class.replace(1, 0)\n",
    "Y_train_3class = Y_train_3class.replace([0.25, 0.75], 1)\n",
    "Y_train_3class = Y_train_3class.replace(0.5, 2)\n",
    "\n",
    "X_train_3class = X_train_3class.to_numpy()\n",
    "Y_train_3class = Y_train_3class.to_numpy()\n",
    "\n",
    "X_train_3class, Y_train_3class = shuffle(X_train_3class, Y_train_3class)\n",
    "\n",
    "#X_train_3class, X_test_3class, Y_train_3class, Y_test_3class = train_test_split(X_train_3class, Y_train_3class, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([0, 1, 2])\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1, 2]), Y_train_3class)\n",
    "w_array = np.ones(Y_train_3class.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(Y_train_3class):\n",
    "    index = np.where(classes == val)\n",
    "    w_array[i] = class_weights[index]\n",
    "    \n",
    "print(class_weights)\n",
    "            \n",
    "evalset = [(X_train_3class, Y_train_3class), (X_test_3class, Y_test_3class)]\n",
    "        \n",
    "earlyStopping = 10\n",
    "  \n",
    "retrainModel = True\n",
    "#model = XGBClassifier(learning_rate=0.3, n_estimators = 1000, max_depth=6, gamma = 5, colsample_bytree=0.5) \n",
    "#[999]\tvalidation_0-mlogloss:0.78046\tvalidation_1-mlogloss:0.97374\n",
    "#model = XGBClassifier(learning_rate=0.3, n_estimators = 1000, max_depth=10, gamma = 5, colsample_bytree=0.5) \n",
    "#[393]\tvalidation_0-mlogloss:0.65456\tvalidation_1-mlogloss:0.95505\n",
    "#model = XGBClassifier(learning_rate=0.1, n_estimators = 1000, max_depth=20, gamma = 5, reg_alpha=0.01, colsample_bytree=0.5)\n",
    "#[522]\tvalidation_0-mlogloss:0.59919\tvalidation_1-mlogloss:0.93002\n",
    "\n",
    "#no weights\n",
    "#model = XGBClassifier(learning_rate=0.1, n_estimators = 1000, max_depth=20, gamma = 5, reg_alpha=0.01, colsample_bytree=0.5)\n",
    "#[76]\tvalidation_0-mlogloss:0.65330\tvalidation_1-mlogloss:0.89610\n",
    "model = XGBClassifier(learning_rate=0.1, n_estimators = 1000, max_depth=6, gamma = 1, reg_alpha=0.01, colsample_bytree=0.5)\n",
    "\n",
    "if retrainModel:\n",
    "    model.fit(X_train_3class, Y_train_3class,  eval_set=evalset, early_stopping_rounds=earlyStopping) #sample_weight=w_array,\n",
    "    retrainModel = False\n",
    "# else:\n",
    "#     model.fit(X_trainE, Y_trainE,  eval_set=evalset, sample_weight=w_array, early_stopping_rounds=earlyStopping, xgb_model='model_eraWise_xgBoost.model')\n",
    "model.save_model('model_0_1_xgBoost.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_3classes = neuralNets.defineNN_3classes(X_val_3class.shape[1])\n",
    "optAdam    = tf.keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.89, beta_2=0.99)\n",
    "modelNN_3classes.compile(optimizer=optAdam, loss='categorical_crossentropy', metrics='categorical_accuracy')\n",
    "modelNN_3classes.load_weights(\"model_3class_99train_97val_4_4_21.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_3class_oneHot = oneHotEncodeData3Classes(Y_val_3class)\n",
    "\n",
    "start_date = date(2021,1, 31)\n",
    "path = \"data/numerai_datasets_\"\n",
    "test_date = start_date\n",
    "test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "epochsPerData =6\n",
    "epochs = 1\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    while os.path.exists(test_path):\n",
    "        path_to_csv = test_path + \"/numerai_training_data.csv\" \n",
    "        tic = time.time()\n",
    "        training_data = pd.read_csv(str(path_to_csv))\n",
    "        toc = time.time()\n",
    "        print(\"loaded data \",toc-tic, \"date = \", test_path)\n",
    "        \n",
    "        if test_date != date(2021, 4, 4):\n",
    "            path_to_csv_val = test_path + \"/numerai_validation_data.csv\" \n",
    "            if os.path.exists(path_to_csv_val):\n",
    "                val_data = pd.read_csv(str(path_to_csv_val))\n",
    "                val_data_3classes = val_data[training_data.columns]\n",
    "                \n",
    "                training_data = pd.concat([training_data, val_data_3classes], axis = 0, ignore_index = True)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "        X_train_3class = training_data[feature_cols].reset_index().drop(['index'], axis = 1).to_numpy()\n",
    "        Y_train_3class = training_data.target        \n",
    "        \n",
    "        Y_train_3class = Y_train_3class.replace(1, 0)\n",
    "        Y_train_3class = Y_train_3class.replace([0.25, 0.75], 1)\n",
    "        Y_train_3class = Y_train_3class.replace(0.5, 2)        \n",
    "        \n",
    "        Y_train_3class_oneHot = oneHotEncodeData3Classes(Y_train_3class)\n",
    "        #Y_test_3class_oneHot  = oneHotEncodeData3Classes(Y_test_3class)\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1, 2]), Y_train_3class)\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "        history = modelNN_3classes.fit(X_train_3class, Y_train_3class_oneHot, epochs = epochsPerData, class_weight=class_weights, batch_size = 256*128, validation_data=(X_val_3class, Y_val_3class_oneHot))\n",
    "\n",
    "        test_date = test_date + delta\n",
    "        test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "#import pdb; pdb.set_trace()\n",
    "#model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_3classes.save_weights(\"model_3class_99train_99val_4_4_21.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelNN_3classes.predict(X_val_3class)\n",
    "pred = np.argmax(pred, axis = 1) #convert one hot vectors to labels\n",
    "\n",
    "accuracy = np.array(Y_val_3class == pred).astype(float).sum() / X_val_3class.shape[0]\n",
    "print(accuracy)\n",
    "predictions_df = pd.DataFrame(pred)\n",
    "#predictions_df[\"pred\"] = pred\n",
    "predictions_df.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelNN_3classes.predict(X_train_3class)\n",
    "pred = np.argmax(pred, axis = 1) #convert one hot vectors to labels\n",
    "\n",
    "accuracy = np.array(Y_train_3class == pred).astype(float).sum() / X_train_3class.shape[0]\n",
    "print(accuracy)\n",
    "predictions_df = pd.DataFrame(pred)\n",
    "#predictions_df[\"pred\"] = pred\n",
    "predictions_df.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df[\"target\"] = Y_train_3class\n",
    "predictions_df.target.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier between 0.25/0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_025 = validation_data.loc[validation_data.target == 0.25]\n",
    "validation_data_075 = validation_data.loc[validation_data.target == 0.75]\n",
    "validation_data_025075 = pd.concat([validation_data_025, validation_data_075], axis = 0, ignore_index = True)\n",
    "\n",
    "\n",
    "X_val_025075 = validation_data_025075[feature_cols].to_numpy()\n",
    "Y_val_025075 = validation_data_025075.target\n",
    "Y_val_025075 = Y_val_025075.replace(0.25, 0)\n",
    "Y_val_025075 = Y_val_025075.replace(0.75, 1)\n",
    "Y_val_025075 = Y_val_025075.to_numpy()\n",
    "\n",
    "X_val_025075, Y_val_025075 = shuffle(X_val_025075, Y_val_025075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_025075 = neuralNets.defineNN_2classes(X_val_025075.shape[1])\n",
    "optAdam    = tf.keras.optimizers.Adam(learning_rate=0.0003, beta_1=0.9, beta_2=0.999)\n",
    "modelNN_025075.compile(optimizer=optAdam, loss='binary_crossentropy', metrics='accuracy')\n",
    "modelNN_025075.load_weights('model_025075_class_96train__96val.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2021,1, 31)\n",
    "path = \"data/numerai_datasets_\"\n",
    "test_date = start_date\n",
    "test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "epochsPerData = 10\n",
    "epochs = 1\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_date = date(2021,1, 31)\n",
    "    path = \"data/numerai_datasets_\"\n",
    "    test_date = start_date\n",
    "    test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "    while os.path.exists(test_path):\n",
    "        path_to_csv = test_path + \"/numerai_training_data.csv\" \n",
    "        tic = time.time()\n",
    "        training_data = pd.read_csv(str(path_to_csv))\n",
    "        toc = time.time()\n",
    "        print(\"loaded data \",toc-tic, \"date = \", test_path)\n",
    "        \n",
    "        training_data_075 = training_data.loc[training_data.target == 0.75]\n",
    "        training_data_025 = training_data.loc[training_data.target == 0.25]\n",
    "        training_data_025075 = pd.concat([training_data_075, training_data_025], axis = 0, ignore_index = True)\n",
    "\n",
    "        if test_date != date(2021, 4, 4):\n",
    "            path_to_csv_val = test_path + \"/numerai_validation_data.csv\" \n",
    "            if os.path.exists(path_to_csv_val):\n",
    "                val_data = pd.read_csv(str(path_to_csv_val))\n",
    "                val_data_075 = val_data.loc[val_data.target == 0.75]\n",
    "                val_data_025 = val_data.loc[val_data.target == 0.25]\n",
    "                val_data_025075 = pd.concat([val_data_075, val_data_025], axis = 0, ignore_index = True)\n",
    "                val_data_025075 = val_data_025075[training_data.columns]\n",
    "                \n",
    "                training_data_025075 = pd.concat([training_data_025075, val_data_025075], axis = 0, ignore_index = True)\n",
    "\n",
    "     \n",
    "        X_train_025075 = val_data_025075[feature_cols].to_numpy()\n",
    "        Y_train_025075 = val_data_025075.target\n",
    "        Y_train_025075 = Y_train_025075.replace(0.25, 0)\n",
    "        Y_train_025075 = Y_train_025075.replace(0.75, 1)\n",
    "        Y_train_025075 = Y_train_025075.to_numpy()\n",
    "        \n",
    "        X_train_025075, Y_train_025075 = shuffle(X_train_025075, Y_train_025075)\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_025075)\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "    \n",
    "        history = modelNN_025075.fit(X_train_025075, Y_train_025075, epochs = epochsPerData, batch_size = 256*128, class_weight=class_weights, validation_data=(X_val_025075, Y_val_025075))\n",
    "\n",
    "        test_date = test_date + delta\n",
    "        test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_025075.save_weights(\"model_025075_class_998train__998val.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_accuracyNP(target, pred):\n",
    "    correctVals = 0\n",
    "    n_examples = target.shape[0]\n",
    "    trueArray = (np.rint(pred) == np.reshape(target, (n_examples,1))).astype(float)\n",
    "    wrongValues = 0\n",
    "    for i in range(len(trueArray)):\n",
    "        if trueArray[i] == 1:\n",
    "            correctVals += 1\n",
    "        elif trueArray[i] == 0:\n",
    "            wrongValues += 1\n",
    "        else:\n",
    "            print(\"weird value \", trueArray[i])\n",
    "#     print(trueArray.shape, pred.shape, target.shape)\n",
    "#     for n in range(len(trueArray)):\n",
    "#         print(pred[n], target[n])\n",
    "    print(\"true\", correctVals,\"wrong\", wrongValues, \"n_examples\", n_examples)\n",
    "    return trueArray.sum()/n_examples    \n",
    "\n",
    "def max_accuracy(target, pred):\n",
    "  \"\"\"Calculates how often the max prediction matches one-hot labels.\"\"\"\n",
    "  retVal = 0\n",
    "  #print(pred.shape[0])\n",
    "  if pred.shape[0] != None:  \n",
    "    num_correct_classified = (tf.math.round(pred) == target)\n",
    "    num = tf.reduce_sum(tf.dtypes.cast(num_correct_classified, tf.int32), axis = -1)\n",
    "    retVal = num / pred.shape[0]\n",
    "    print(tf.math.round(pred))#, num_currect_classified, retVal)\n",
    "  else:\n",
    "    retVal = tf.dtypes.cast(tf.math.round(pred) == target, tf.int32)\n",
    "  print(retVal)\n",
    "  return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelNN_025075.predict(X_val_025075)\n",
    "\n",
    "print(max_accuracyNP(Y_val_025075, pred))\n",
    "\n",
    "predictions_df = pd.DataFrame(pred)\n",
    "#predictions_df[\"pred\"] = pred\n",
    "predictions_df.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 = training_data.loc[training_data.target == 1]\n",
    "training_data_0 = training_data.loc[training_data.target == 0]\n",
    "training_data_01 = pd.concat([training_data_1, training_data_0], axis = 0, ignore_index = True)\n",
    "\n",
    "X_train_01 = training_data_01[feature_cols].to_numpy()\n",
    "Y_train_01 = training_data_01.target.to_numpy()\n",
    "X_train_01, Y_train_01 = shuffle(X_train_01, Y_train_01)\n",
    "\n",
    "X_train_01, X_test_01, Y_train_01, Y_test_01 = train_test_split(X_train_01, Y_train_01, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_1 = validation_data.loc[validation_data.target == 1]\n",
    "validation_data_0 = validation_data.loc[validation_data.target == 0]\n",
    "validation_data_01 = pd.concat([validation_data_1, validation_data_0], axis = 0, ignore_index = True)\n",
    "\n",
    "X_val_01 = validation_data_01[feature_cols].to_numpy()\n",
    "Y_val_01 = validation_data_01.target.to_numpy()\n",
    "#X_val_01, Y_val_01 = shuffle(X_val_01, Y_val_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array([0, 1])\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_01)\n",
    "w_array = np.ones(Y_train_01.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(Y_train_01):\n",
    "    index = np.where(classes == val)\n",
    "    w_array[i] = class_weights[index]\n",
    "    \n",
    "print(class_weights)\n",
    "            \n",
    "evalset = [(X_train_01, Y_train_01), (X_val_01, Y_val_01)]\n",
    "        \n",
    "earlyStopping = 10\n",
    "  \n",
    "retrainModel = True\n",
    "model = XGBClassifier(learning_rate=0.005, n_estimators = 1000, max_depth=10, gamma = 25)#, colsample_bytree=0.1)    \n",
    "\n",
    "if retrainModel:\n",
    "    model.fit(X_train_01, Y_train_01,  eval_set=evalset, sample_weight=w_array, early_stopping_rounds=earlyStopping)\n",
    "    retrainModel = False\n",
    "# else:\n",
    "#     model.fit(X_trainE, Y_trainE,  eval_set=evalset, sample_weight=w_array, early_stopping_rounds=earlyStopping, xgb_model='model_eraWise_xgBoost.model')\n",
    "model.save_model('model_0_1_xgBoost.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineNN_2classes(n_inputFeatures):\n",
    "    activation = \"relu\"\n",
    "    regularizationConst_l1 = 0.00000#3\n",
    "    regularizationConst_l2 = 0.00000#3\n",
    "    size = 512\n",
    "    X_input = Input(shape=(n_inputFeatures,))\n",
    "    X = Dropout(0.00, input_shape = (n_inputFeatures,))(X_input)\n",
    " \n",
    "    X = Dense(size, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "    #X = Dropout(dropoutRate, input_shape = (size,))(X)\n",
    "    #X = BatchNormalization(axis = -1)(X)\n",
    "    X = Dense(size, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "    #X = Dropout(dropoutRate, input_shape = (size,))(X)\n",
    "    #X = BatchNormalization(axis = -1)(X)\n",
    "    X = Dense(size, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "    X = Dense(size, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "    X = Dense(size, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "\n",
    "    #X = Dropout(dropoutRate, input_shape = (size,))(X)\n",
    "    #X = BatchNormalization(axis = -1)(X)\n",
    "    X = Dense(size/4, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "    X = Dense(size/8, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "    X = Dense(size/16, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "    X = Dense(size/32, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "\n",
    "    X = Dense(1, activation=\"sigmoid\")(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name='deepNN')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "modelNN_01 = defineNN_2classes(X_train_01.shape[1])\n",
    "optAdam    = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99)\n",
    "modelNN_01.compile(optimizer=optAdam, loss='binary_crossentropy', metrics='accuracy')\n",
    "#modelNN_01.load_weights('model_01class_97train_97test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_orig = date(2021,1, 31)\n",
    "#start_date_orig = date(2021,3, 28)\n",
    "path = \"data/numerai_datasets_\"\n",
    "test_date = start_date_orig\n",
    "test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "epochsPerData = 25\n",
    "epochs = 1\n",
    "counter = 0\n",
    "epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_date = start_date_orig\n",
    "    path = \"data/numerai_datasets_\"\n",
    "    test_date = start_date\n",
    "    test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "    while os.path.exists(test_path):\n",
    "        path_to_csv = test_path + \"/numerai_training_data.csv\" \n",
    "        tic = time.time()\n",
    "        training_data = pd.read_csv(str(path_to_csv))\n",
    "        toc = time.time()\n",
    "        print(\"loaded data \",toc-tic, \"date = \", test_path)\n",
    "\n",
    "        \n",
    "        training_data_1 = training_data.loc[training_data.target == 1]\n",
    "        training_data_0 = training_data.loc[training_data.target == 0]\n",
    "        training_data_01 = pd.concat([training_data_1, training_data_0], axis = 0, ignore_index = True)\n",
    "        \n",
    "        if test_date != date(2021, 4, 4):\n",
    "            path_to_csv_val = test_path + \"/numerai_validation_data.csv\" \n",
    "            if os.path.exists(path_to_csv_val):\n",
    "                val_data = pd.read_csv(str(path_to_csv_val))\n",
    "                val_data_1 = val_data.loc[val_data.target == 1]\n",
    "                val_data_0 = val_data.loc[val_data.target == 0]\n",
    "                val_data_01 = pd.concat([val_data_1, val_data_0], axis = 0, ignore_index = True)\n",
    "                val_data_01 = val_data_01[training_data.columns]\n",
    "                training_data_01 = pd.concat([training_data_01, val_data_01], axis = 0, ignore_index = True)\n",
    "        \n",
    "        #append all training data into one dataframe\n",
    "        if counter == 0:\n",
    "            appendedTraining_data_01 = training_data_01\n",
    "        else:\n",
    "            appendedTraining_data_01 = pd.concat([training_data_01, appendedTraining_data_01], axis = 0, ignore_index = True)\n",
    "        \n",
    "#         X_train_01 = training_data_01[feature_cols].to_numpy()\n",
    "#         Y_train_01 = training_data_01.target.to_numpy()\n",
    "#         X_train_01, Y_train_01 = shuffle(X_train_01, Y_train_01)\n",
    "        \n",
    "#         class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_01)\n",
    "#         class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "    \n",
    "#        history = modelNN_01.fit(X_train_01, Y_train_01, epochs = epochsPerData, batch_size = 256*128*2, class_weight=class_weights, validation_data=(X_test_01, Y_test_01))\n",
    "\n",
    "        test_date = test_date + delta\n",
    "        test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "        counter += 1\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #model.save_weights(\"model.h5\")\n",
    " \n",
    "\n",
    "# train with one appended matrix\n",
    "print(appendedTraining_data_01.shape)\n",
    "X_train_01 = appendedTraining_data_01[feature_cols].to_numpy()\n",
    "Y_train_01 = appendedTraining_data_01.target.to_numpy()\n",
    "X_train_01, Y_train_01 = shuffle(X_train_01, Y_train_01)\n",
    "        \n",
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_01)\n",
    "class_weights = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = modelNN_01.fit(X_train_01, Y_train_01, epochs = 5, batch_size = 256*128*2, class_weight=class_weights, validation_data=(X_val_01, Y_val_01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendedTraining_data_01.to_csv(\"appendedTrainingData_classes_0_1_till_04_04_21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_01.save_weights(\"model_01class_99train_99test_994val.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "#print(history.history)\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelNN_01.predict(X_val_01)\n",
    "\n",
    "print(max_accuracyNP(Y_val_01, pred))\n",
    "\n",
    "predictions_df = pd.DataFrame(pred)\n",
    "#predictions_df[\"pred\"] = pred\n",
    "predictions_df.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true = np.array([0, 1, 0])\n",
    "test_pred = np.array([0.4, 0.500000001, 0.1])\n",
    "\n",
    "\n",
    "max_accuracy(test_true, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelNN_01.predict(X_train_01)\n",
    "#pred2 = modelNN.predict(X_test_01)\n",
    "\n",
    "print(max_accuracyNP(Y_train_01, pred))\n",
    "\n",
    "predictions_df = pd.DataFrame(pred)\n",
    "#predictions_df[\"pred\"] = pred\n",
    "predictions_df.hist(bins = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
