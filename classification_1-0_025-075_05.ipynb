{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "\n",
    "import neuralNets\n",
    "\n",
    "def oneHotEncodeData3Classes(targets):\n",
    "    j=0\n",
    "    Y_val = np.zeros((targets.shape[0], 3))\n",
    "    for j in range(targets.shape[0]):\n",
    "        if targets[j] == 0:\n",
    "            Y_val[j, 0] = 1\n",
    "        elif targets[j] == 1:\n",
    "            Y_val[j, 1] = 1\n",
    "        elif targets[j] == 2:\n",
    "            Y_val[j, 2] = 1\n",
    "        else:\n",
    "            print(\"something went wrong, new class\", targets[j])\n",
    "    return Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "training_data = pd.read_csv(\"data/numerai_datasets_02.05.21/numerai_training_data.csv\")\n",
    "feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n",
    "\n",
    "training_data[feature_cols] = training_data[feature_cols].astype(np.float16)\n",
    "training_data.target        = training_data.target.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(\"data/numerai_datasets_02.05.21/numerai_validation_data.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier between 0/1 0.25/0.75 and 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3class = training_data[feature_cols]\n",
    "Y_train_3class = training_data.target\n",
    "\n",
    "Y_train_3class = Y_train_3class.replace(1, 0)\n",
    "Y_train_3class = Y_train_3class.replace([0.25, 0.75], 1)\n",
    "Y_train_3class = Y_train_3class.replace(0.5, 2)\n",
    "\n",
    "X_train_3class = X_train_3class.to_numpy()\n",
    "Y_train_3class = Y_train_3class.to_numpy()\n",
    "\n",
    "X_train_3class, Y_train_3class = shuffle(X_train_3class, Y_train_3class)\n",
    "\n",
    "X_train_3class, X_test_3class, Y_train_3class, Y_test_3class = train_test_split(X_train_3class, Y_train_3class, test_size = 0.3)\n",
    "\n",
    "X_val_3class = validation_data[feature_cols]\n",
    "Y_val_3class = validation_data.target\n",
    "\n",
    "Y_val_3class = Y_val_3class.replace(1, 0)\n",
    "Y_val_3class = Y_val_3class.replace([0.25, 0.75], 1)\n",
    "Y_val_3class = Y_val_3class.replace(0.5, 2)\n",
    "\n",
    "X_val_3class = X_val_3class.to_numpy()\n",
    "Y_val_3class = Y_val_3class.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_3classes = neuralNets.defineNN_3classes(X_val_3class.shape[1])\n",
    "optAdam    = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.99)\n",
    "\n",
    "modelNN_3classes.compile(optimizer=optAdam, loss='categorical_crossentropy', metrics='categorical_accuracy')\n",
    "modelNN_3classes.load_weights(\"model_3_overfit_100train_100val.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_3class_oneHot = oneHotEncodeData3Classes(Y_train_3class)\n",
    "Y_test_3class_oneHot  = oneHotEncodeData3Classes(Y_test_3class)\n",
    "Y_val_3class_oneHot   = oneHotEncodeData3Classes(Y_val_3class)\n",
    "\n",
    "test_history = []\n",
    "val_history = []\n",
    "class MyCustomCallback_3class(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        res_eval_1 = self.model.evaluate(X_test_3class, Y_test_3class_oneHot, verbose = 0)\n",
    "        res_eval_2 = self.model.evaluate(X_val_3class, Y_val_3class_oneHot, verbose = 0)\n",
    "        test_history.append(res_eval_1[0])\n",
    "        val_history.append(res_eval_2[0])\n",
    "        print(\"test \",res_eval_1)\n",
    "        print(\"val\", res_eval_2)\n",
    "my_val_callback_3class = MyCustomCallback_3class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1, 2]), Y_train_3class)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "test_history = []\n",
    "val_history = []\n",
    "history = modelNN_3classes.fit(X_train_3class, Y_train_3class_oneHot, epochs = 100, class_weight=class_weights, batch_size = 128*256, callbacks = [my_val_callback_3class])#validation_data=(X_test_3class, Y_test_3class_oneHot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_3classes.save_weights(\"model_3class_100train_92val_noValData.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = test_history\n",
    "val_loss = val_history\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "#print(history.history)\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.plot(epoch_count, val_loss, 'g--')\n",
    "plt.legend(['Training Loss', 'Test Loss', 'val loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier between 0.25/0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_025 = validation_data.loc[validation_data.target == 0.25]\n",
    "validation_data_075 = validation_data.loc[validation_data.target == 0.75]\n",
    "validation_data_025075 = pd.concat([validation_data_025, validation_data_075], axis = 0, ignore_index = True)\n",
    "\n",
    "\n",
    "X_val_025075 = validation_data_025075[feature_cols].to_numpy()\n",
    "Y_val_025075 = validation_data_025075.target\n",
    "Y_val_025075 = Y_val_025075.replace(0.25, 0)\n",
    "Y_val_025075 = Y_val_025075.replace(0.75, 1)\n",
    "Y_val_025075 = Y_val_025075.to_numpy()\n",
    "\n",
    "# X_val_025075, Y_val_025075 = shuffle(X_val_025075, Y_val_025075)\n",
    "\n",
    "# training_data_025 = training_data.loc[training_data.target == 0.25]\n",
    "# training_data_075 = training_data.loc[training_data.target == 0.75]\n",
    "# training_data_025075 = pd.concat([training_data_025, training_data_075], axis = 0, ignore_index = True)\n",
    "\n",
    "\n",
    "# X_train_025075 = training_data_025075[feature_cols].to_numpy()\n",
    "# Y_train_025075 = training_data_025075.target\n",
    "# Y_train_025075 = Y_train_025075.replace(0.25, 0)\n",
    "# Y_train_025075 = Y_train_025075.replace(0.75, 1)\n",
    "# Y_train_025075 = Y_train_025075.to_numpy()\n",
    "\n",
    "# X_train_025075, Y_train_025075 = shuffle(X_train_025075, Y_train_025075)\n",
    "\n",
    "# X_train_025075, X_test_025075, Y_train_025075, Y_test_025075 = train_test_split(X_train_025075, Y_train_025075, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_025075)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_025075 = neuralNets.defineNN_2classes(X_train_025075.shape[1])\n",
    "optAdam    = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.99, beta_2=0.99999)\n",
    "modelNN_025075.compile(optimizer=optAdam, loss='binary_crossentropy', metrics='accuracy')\n",
    "modelNN_025075.load_weights('model_025075_overfit_99train_99val.h5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback_025075(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        res_eval_1 = self.model.evaluate(X_test_025075, Y_test_025075, verbose = 0)\n",
    "        res_eval_2 = self.model.evaluate(X_val_025075, Y_val_025075, verbose = 0)\n",
    "        print(\"test \",res_eval_1)\n",
    "        print(\"val\", res_eval_2)\n",
    "my_val_callback_025075 = MyCustomCallback_025075()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = modelNN_025075.fit(X_train_025075, Y_train_025075, epochs = 50, batch_size = 256*128*10, class_weight=class_weights, callbacks = [my_val_callback_025075])   #validation_data=(X_val_025075, Y_val_025075))#(X_test_025075, Y_test_025075)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_025075.save_weights(\"model_025075_class_100train_92test_noValData.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "#print(history.history)\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 = training_data.loc[training_data.target == 1]\n",
    "training_data_0 = training_data.loc[training_data.target == 0]\n",
    "training_data_01 = pd.concat([training_data_1, training_data_0], axis = 0, ignore_index = True)\n",
    "\n",
    "X_train_01 = training_data_01[feature_cols].to_numpy()\n",
    "Y_train_01 = training_data_01.target.to_numpy()\n",
    "X_train_01, Y_train_01 = shuffle(X_train_01, Y_train_01)\n",
    "\n",
    "X_train_01, X_test_01, Y_train_01, Y_test_01 = train_test_split(X_train_01, Y_train_01, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_1 = validation_data.loc[validation_data.target == 1]\n",
    "validation_data_0 = validation_data.loc[validation_data.target == 0]\n",
    "validation_data_01 = pd.concat([validation_data_1, validation_data_0], axis = 0, ignore_index = True)\n",
    "\n",
    "X_val_01 = validation_data_01[feature_cols].to_numpy()\n",
    "Y_val_01 = validation_data_01.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history = []\n",
    "val_history = []\n",
    "class MyCustomCallback_01(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        res_eval_1 = self.model.evaluate(X_test_01, Y_test_01, verbose = 0)\n",
    "        res_eval_2 = self.model.evaluate(X_val_01, Y_val_01, verbose = 0)\n",
    "        test_history.append(res_eval_1[0])\n",
    "        val_history.append(res_eval_2[0])\n",
    "        print(\"test \",res_eval_1)\n",
    "        print(\"val\", res_eval_2)\n",
    "my_val_callback_01 = MyCustomCallback_01()\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_01)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_01 = neuralNets.defineNN_small_2classes(X_train_01.shape[1])\n",
    "optAdam    = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.99)\n",
    "\n",
    "modelNN_01.compile(optimizer=optAdam, loss='binary_crossentropy', metrics='accuracy')\n",
    "modelNN_01.load_weights('model_01_overfit_99train_99val.h5.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history = []\n",
    "val_history = []\n",
    "history = modelNN_01.fit(X_train_01, Y_train_01, epochs = 500, batch_size = 256*128, class_weight=class_weights, callbacks = [my_val_callback_01]) #validation_data=(X_val_01, Y_val_01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_01.save_weights(\"model_01class_100train_95val_noValData.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = test_history\n",
    "val_loss = val_history\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "#print(history.history)\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.plot(epoch_count, val_loss, 'g--')\n",
    "plt.legend(['Training Loss', 'Test Loss', 'val loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
