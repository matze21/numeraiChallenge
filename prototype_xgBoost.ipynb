{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import numerapi\n",
    "import sklearn.linear_model\n",
    "\n",
    "# import keras_deepCNN\n",
    "# from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "# from tensorflow.keras.models import Model, load_model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "# gradient boosting for classification in scikit-learn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import pyplot\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useValidationData=True\n",
    "\n",
    "tic = time.time()\n",
    "# training_data = pd.read_csv(\"balancedTrainingData_test12021-01-31_2021-03-21.csv\")\n",
    "\n",
    "# feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n",
    "# X_train = training_data[feature_cols].to_numpy()\n",
    "\n",
    "# #Y_train = training_data.loc[:,['label_0', 'label_025', 'label_05', 'label_075', 'label_1']].to_numpy()\n",
    "# Y_train = training_data.loc[:,['target']].to_numpy()\n",
    "\n",
    "#training_data = pd.read_csv(\"appendedTrainingData_test12021-02-28_2021-03-28.csv\")\n",
    "training_data = pd.read_csv(\"data/numerai_datasets_28.03.21/numerai_training_data.csv\")\n",
    "\n",
    "feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n",
    "X_train = training_data[feature_cols].reset_index().drop(['index'], axis = 1)\n",
    "\n",
    "\n",
    "X_train_0 = X_train.to_numpy()\n",
    "Y_train_0 = training_data.target\n",
    "# Y_train_0 = Y_train_0.replace(0.5, 2)\n",
    "# Y_train_0 = Y_train_0.replace([0.25, 0, 0.75, 1], 0)\n",
    "# Y_train_0 = Y_train_0.replace(2, 1)\n",
    "Y_train_0 = Y_train_0.to_numpy()\n",
    "\n",
    "if useValidationData:\n",
    "    tournament_data = pd.read_csv(\"data/numerai_datasets_28.03.21/numerai_tournament_data.csv\")\n",
    "    validation_data = tournament_data.loc[tournament_data.data_type == 'validation']\n",
    "    X_val = validation_data[feature_cols].reset_index().drop(['index'], axis = 1).to_numpy()\n",
    "    Y_val = validation_data.target.to_numpy()\n",
    "    Y_val_0 = Y_val\n",
    "    X_val_0 = X_val\n",
    "    # Y_val_0 = Y_val.replace(0.5, 2)\n",
    "    # Y_val_0 = Y_val_0.replace([0.25, 0, 0.75, 1], 0)\n",
    "    # Y_val_0 = Y_val_0.replace(2, 1)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"loaded the data took \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_0 = training_data.target\n",
    "# 0, 1       -> 0\n",
    "# 0.5        -> 1\n",
    "# 0.25, 0.75 -> 2\n",
    "\n",
    "# Y_train_0 = Y_train_0.replace([0.25, 0.75], 2)\n",
    "# Y_train_0 = Y_train_0.replace([0, 1], 0)\n",
    "# Y_train_0 = Y_train_0.replace(0.5, 1)\n",
    "#Y_train_0 = Y_train_0.replace(2, 1)\n",
    "\n",
    "feature_cols_wisdom = training_data.columns[training_data.columns.str.startswith('feature_w')]\n",
    "feature_cols_consti = training_data.columns[training_data.columns.str.startswith('feature_con')]\n",
    "feature_cols_streng = training_data.columns[training_data.columns.str.startswith('feature_str')]\n",
    "feature_cols_charis = training_data.columns[training_data.columns.str.startswith('feature_cha')]\n",
    "feature_cols_intell = training_data.columns[training_data.columns.str.startswith('feature_int')]\n",
    "\n",
    "X_train = training_data[feature_cols].reset_index().drop(['index'], axis = 1)\n",
    "\n",
    "Y_val_0 = validation_data.target#.replace([0.25, 0.75], 2)\n",
    "# Y_val_0 = Y_val_0.replace([0, 1], 0)\n",
    "# Y_val_0 = Y_val_0.replace(0.5, 1)\n",
    "\n",
    "X_val = validation_data[feature_cols].reset_index().drop(['index'], axis = 1).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, Y_train = shuffle(X_train.to_numpy(), training_data.target.to_numpy())\n",
    "\n",
    "X_train, Y_train = shuffle(X_train.to_numpy(), Y_train_0.to_numpy())\n",
    "Y_val = Y_val_0.to_numpy()\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# Y_train = le.fit_transform(Y_train)\n",
    "# Y_val = le.fit_transform(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "#classes = np.array([0, 1, 2])\n",
    "classes = np.array([0, 0.25, 0.5, 0.75, 1])\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes, Y_train)\n",
    "#class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "w_array = np.ones(Y_train.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(Y_train):\n",
    "    index = np.where(classes == val)\n",
    "    w_array[i] = class_weights[index]\n",
    "    \n",
    "evalset = [(X_train, Y_train), (X_val,Y_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(\n",
    "        #base_score=0.0, booster='gbtree', colsample_bylevel=1,\n",
    "    #    colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
    "    #    max_delta_step=0.0, max_depth=8, min_child_weight=1, missing=None,\n",
    "        n_estimators=1000, #n_jobs=10, nthread=None,\n",
    "    #    objective='multi:softprob',# random_state=0, \n",
    "    #    reg_alpha= 0, reg_lambda = 0,\n",
    "    #    reg_lambda=1.0, scale_pos_weight=1, seed=None, silent=None,\n",
    "    #    subsample=0.8, verbosity=0\n",
    "     #   eta = 0.05,\n",
    "        #subsample=0.5, colsample_bytree=0.5, colsample_bylevel=0.5,\n",
    "#         max_depth = 20,\n",
    "#         gamma = 10,\n",
    "#         min_child_weight = 5\n",
    "    colsample_bytree=0.1\n",
    "       ) \n",
    "model.fit(X_train, Y_train,  eval_set=evalset, sample_weight=w_array, early_stopping_rounds=50) #xgb_model='model_kFold.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 classes + default xgb\n",
    "[1471]\tvalidation_0-mlogloss:0.52806\tvalidation_1-mlogloss:0.96262\n",
    "3 classes + only wisdom feature\n",
    "[762]\tvalidation_0-mlogloss:0.86781\tvalidation_1-mlogloss:1.02326\n",
    "        \n",
    "5 classes + only wisdom\n",
    "[1615]\tvalidation_0-mlogloss:1.05126\tvalidation_1-mlogloss:1.49122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('model_938Train_406Val.model')  #val loss end 1.46256, mit gamma = 0, loss = 1.465, gamma = 10, loss = 1.447\n",
    "\n",
    "# gamma = 0, depth = 10\n",
    "# [100]\tvalidation_0-mlogloss:0.65688\tvalidation_1-mlogloss:1.36347\n",
    "# gamma = 20, min_child_weight = 1\n",
    "# [100]\tvalidation_0-mlogloss:1.41976\tvalidation_1-mlogloss:1.47285\n",
    "# gamma = 20, min_child_weight = 2\n",
    "# [100]\tvalidation_0-mlogloss:1.42026\tvalidation_1-mlogloss:1.47273  diff = 0.0521\n",
    "# gamma = 20, min_child_weight = 5      \n",
    "# [100]\tvalidation_0-mlogloss:1.41991\tvalidation_1-mlogloss:1.47209  diff = 0.0529\n",
    "# gamma = 20, min_child_weight = 1.5\n",
    "# [100]\tvalidation_0-mlogloss:1.41930\tvalidation_1-mlogloss:1.47213\n",
    "\n",
    "# #gamma = 2\n",
    "# [100]\tvalidation_0-mlogloss:1.40372\tvalidation_1-mlogloss:1.48010\n",
    "# #gamma = 1\n",
    "# [100]\tvalidation_0-mlogloss:1.40386\tvalidation_1-mlogloss:1.47948\n",
    "# #gamma = 0.5\n",
    "# [100]\tvalidation_0-mlogloss:1.40408\tvalidation_1-mlogloss:1.48020\n",
    "# #gamma = 0.75\n",
    "# [100]\tvalidation_0-mlogloss:1.40383\tvalidation_1-mlogloss:1.47926\n",
    "# #gamma = 0.825\n",
    "# [100]\tvalidation_0-mlogloss:1.40383\tvalidation_1-mlogloss:1.47926\n",
    "        \n",
    "# #min_child_weight = 2\n",
    "# [100]\tvalidation_0-mlogloss:1.40407\tvalidation_1-mlogloss:1.48185\n",
    "# #min_child_weight = 0.75\n",
    "# [100]\tvalidation_0-mlogloss:1.40398\tvalidation_1-mlogloss:1.48077\n",
    "# #min_child_weight = 0.75       \n",
    "# [100]\tvalidation_0-mlogloss:1.40387\tvalidation_1-mlogloss:1.48020\n",
    "# #min_child_weight = 1.25       \n",
    "# [100]\tvalidation_0-mlogloss:1.40376\tvalidation_1-mlogloss:1.48066\n",
    "# #min_child_weight = 5 \n",
    "# [100]\tvalidation_0-mlogloss:1.40439\tvalidation_1-mlogloss:1.48035\n",
    "# #min_child_weight = 50     \n",
    "# [100]\tvalidation_0-mlogloss:1.40725\tvalidation_1-mlogloss:1.48065\n",
    "# #min_child_weight = 10\n",
    "# [100]\tvalidation_0-mlogloss:1.40439\tvalidation_1-mlogloss:1.48035\n",
    "# #min_child_weight = 0.5      \n",
    "# [100]\tvalidation_0-mlogloss:1.40379\tvalidation_1-mlogloss:1.48014\n",
    "# #0.2\n",
    "# [100]\tvalidation_0-mlogloss:1.40377\tvalidation_1-mlogloss:1.48040\n",
    "# #1.1\n",
    "# [100]\tvalidation_0-mlogloss:1.40363\tvalidation_1-mlogloss:1.48024\n",
    "# 1\n",
    "# [100]\tvalidation_0-mlogloss:1.40383\tvalidation_1-mlogloss:1.47926\n",
    "# 0.9\n",
    "# [100]\tvalidation_0-mlogloss:1.40350\tvalidation_1-mlogloss:1.48004\n",
    "# [400]\tvalidation_0-mlogloss:1.16978\tvalidation_1-mlogloss:1.44122\n",
    "        \n",
    "# lambda = 0.1, alpha = 0.1\n",
    "# [100]\tvalidation_0-mlogloss:1.40338\tvalidation_1-mlogloss:1.48019\n",
    "# lambda = 1, alpha = 1\n",
    "# [100]\tvalidation_0-mlogloss:1.40506\tvalidation_1-mlogloss:1.47972\n",
    "# lambda = 10, alpha = 10\n",
    "# [100]\tvalidation_0-mlogloss:1.41773\tvalidation_1-mlogloss:1.48155\n",
    "# lambda = 10, alpha = 2\n",
    "# [100]\tvalidation_0-mlogloss:1.41656\tvalidation_1-mlogloss:1.48019\n",
    "# lambda = 1, alpha = 0.5\n",
    "# [100]\tvalidation_0-mlogloss:1.40506\tvalidation_1-mlogloss:1.47954\n",
    "# lambda = 0, alpha = 1\n",
    "# [100]\tvalidation_0-mlogloss:1.40475\tvalidation_1-mlogloss:1.47962\n",
    "# lambda = 0, alpha = 0\n",
    "# [100]\tvalidation_0-mlogloss:1.40357\tvalidation_1-mlogloss:1.47998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evals_result()\n",
    "# plot learning curves\n",
    "pyplot.plot(results['validation_0']['mlogloss'], label='train')\n",
    "pyplot.plot(results['validation_1']['mlogloss'], label='test')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred = model.predict(X_train)\n",
    "corVal = 0\n",
    "allTrainSamples = len(Y_train_pred)\n",
    "for i in range(allTrainSamples):\n",
    "    if Y_train_pred[i] == Y_train[i]:\n",
    "        corVal +=1\n",
    "accuracy = corVal / allTrainSamples\n",
    "\n",
    "print(\"train accuracy = \", accuracy)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "corVal = 0\n",
    "allTrainSamples = len(y_pred)\n",
    "for i in range(allTrainSamples):\n",
    "    if y_pred[i] == Y_val[i]:\n",
    "        corVal +=1\n",
    "accuracy = corVal / allTrainSamples\n",
    "\n",
    "print(\"validation accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 classifiers + no weights\n",
    "Y_train_pred = model.predict(X_train)\n",
    "corVal = 0\n",
    "allTrainSamples = len(Y_train_pred)\n",
    "for i in range(allTrainSamples):\n",
    "    if Y_train_pred[i] == Y_train[i]:\n",
    "        corVal +=1\n",
    "accuracy = corVal / allTrainSamples\n",
    "\n",
    "print(\"train accuracy = \", accuracy)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "corVal = 0\n",
    "allTrainSamples = len(y_pred)\n",
    "for i in range(allTrainSamples):\n",
    "    if y_pred[i] == Y_val[i]:\n",
    "        corVal +=1\n",
    "accuracy = corVal / allTrainSamples\n",
    "\n",
    "print(\"validation accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1000 classifiers + weights\n",
    "Y_train_pred = model.predict(X_train)\n",
    "corVal = 0\n",
    "allTrainSamples = len(Y_train_pred)\n",
    "for i in range(allTrainSamples):\n",
    "    if Y_train_pred[i] == Y_train[i]:\n",
    "        corVal +=1\n",
    "accuracy = corVal / allTrainSamples\n",
    "\n",
    "print(\"train accuracy = \", accuracy)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "corVal = 0\n",
    "allTrainSamples = len(y_pred)\n",
    "for i in range(allTrainSamples):\n",
    "    if y_pred[i] == Y_val[i]:\n",
    "        corVal +=1\n",
    "accuracy = corVal / allTrainSamples\n",
    "\n",
    "print(\"validation accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1000 classifiers + no weights\n",
    "Y_train_pred = model.predict(X_train)\n",
    "corVal = 0\n",
    "allTrainSamples = len(Y_train_pred)\n",
    "for i in range(allTrainSamples):\n",
    "    if Y_train_pred[i] == Y_train[i]:\n",
    "        corVal +=1\n",
    "accuracy = corVal / allTrainSamples\n",
    "\n",
    "print(\"train accuracy = \", accuracy)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "corVal = 0\n",
    "allTrainSamples = len(y_pred)\n",
    "for i in range(allTrainSamples):\n",
    "    if y_pred[i] == Y_val[i]:\n",
    "        corVal +=1\n",
    "accuracy = corVal / allTrainSamples\n",
    "\n",
    "print(\"validation accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_data_recent = pd.read_csv(\"data/numerai_datasets_28.03.21/numerai_tournament_data.csv\")\n",
    "validation_data_recent = tournament_data_recent.loc[tournament_data_recent.data_type == 'validation']\n",
    "X_val_r = validation_data_recent[feature_cols].reset_index().drop(['index'], axis = 1).to_numpy()\n",
    "Y_val_r = validation_data_recent.target\n",
    "Y_val_r = Y_val_r.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5000 classifiers + no weights\n",
    "# Y_train_pred = model.predict(X_train)\n",
    "# corVal = 0\n",
    "# allTrainSamples = len(Y_train_pred)\n",
    "# for i in range(allTrainSamples):\n",
    "#     if Y_train_pred[i] == Y_train[i]:\n",
    "#         corVal +=1\n",
    "# accuracy = corVal / allTrainSamples\n",
    "\n",
    "# print(\"train accuracy = \", accuracy)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_val_r)\n",
    "corVal = 0\n",
    "allTrainSamples = len(y_pred)\n",
    "for i in range(allTrainSamples):\n",
    "    if y_pred[i] == Y_val_r[i]:\n",
    "        corVal +=1\n",
    "#     else:\n",
    "#         print(\"predicted = \", y_pred[i], \"ground truth = \", Y_val[i])\n",
    "accuracy = corVal / allTrainSamples\n",
    "\n",
    "print(\"validation accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = validation_data[\"id\"].to_frame()\n",
    "predictions_df[\"pred\"] = y_pred\n",
    "predictions_df.pred.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = training_data[\"id\"].to_frame()\n",
    "training_df[\"train_pred\"] = Y_train_pred\n",
    "training_df.train_pred.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.target.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation score\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "scores = accuracy_score(Y_val_0, y_pred)\n",
    "print(\"crossvalidation score\", scores)\n",
    "\n",
    "cm = metrics.confusion_matrix(Y_val_0, y_pred)\n",
    "print(\"confusion matrix\", cm)\n",
    "cr = metrics.classification_report(Y_val_0, y_pred)\n",
    "print(\"classification report\", cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "joblib_file = \"joblib_model_gradientBoosting_99train_46dev.pkl\"\n",
    "joblib.dump(model, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
