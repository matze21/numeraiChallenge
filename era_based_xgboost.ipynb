{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "#import numerapi\n",
    "import sklearn.linear_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "\n",
    "# gradient boosting for classification in scikit-learn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import pyplot\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X_train, Y_train):\n",
    "    tic = time.time()\n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    toc = time.time()\n",
    "    print(\"shuffle the data took \", toc - tic)\n",
    "    return X_train, Y_train\n",
    "\n",
    "def calculate_weights(Y_train):\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.array([0, 0.25, 0.5, 0.75, 1]), Y_train)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "    return class_weights\n",
    "\n",
    "def max_accuracy(y_true, y_pred):\n",
    "  \"\"\"Calculates how often the max prediction matches one-hot labels.\"\"\"\n",
    "  retVal = 0\n",
    "  \n",
    "  if y_true.shape[0] != None:  \n",
    "    num_correct_classified = tf.math.argmax(y_true,axis = 1) == tf.math.argmax(y_pred, axis = 1)\n",
    "    num = tf.reduce_sum(tf.dtypes.cast(num_correct_classified, tf.int32), axis = -1)\n",
    "    retVal = num / y_true.shape[0]\n",
    "  else:\n",
    "    retVal = tf.dtypes.cast(tf.math.argmax(y_true,axis = 1) == tf.math.argmax(y_pred, axis = 1), tf.int32)\n",
    "  return retVal\n",
    "\n",
    "def defineNN(n_inputFeatures):\n",
    "    activation = \"relu\"\n",
    "    regularizationConst_l1 = 0.0002\n",
    "    regularizationConst_l2 = 0.0001\n",
    "    size = 512\n",
    "    X_input = Input(shape=(n_inputFeatures,))\n",
    "    X = Dropout(0.3, input_shape = (n_inputFeatures,))(X_input)\n",
    " \n",
    "    X = Dense(size, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "    #X = Dropout(dropoutRate, input_shape = (size,))(X)\n",
    "    X = BatchNormalization(axis = -1)(X)\n",
    "    X = Dense(size, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "    #X = Dropout(dropoutRate, input_shape = (size,))(X)\n",
    "    X = BatchNormalization(axis = -1)(X)\n",
    "    X = Dense(size, activation=activation, kernel_regularizer=regularizers.l1_l2(l1=regularizationConst_l1, l2=regularizationConst_l2), bias_regularizer=regularizers.l2(regularizationConst_l2), activity_regularizer=regularizers.l2(regularizationConst_l2))(X)\n",
    "    #X = Dropout(dropoutRate, input_shape = (size,))(X)\n",
    "    X = BatchNormalization(axis = -1)(X)\n",
    "    X = Dense(5, activation=\"softmax\")(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name='deepNN')\n",
    "\n",
    "    return model\n",
    "\n",
    "def oneHotEncodeData(targets):\n",
    "    j=0\n",
    "    Y_val = np.zeros((targets.shape[0], 5))\n",
    "    for j in range(targets.shape[0]):\n",
    "        if targets[j] == 0:\n",
    "            Y_val[j, 0] = 1\n",
    "        elif targets[j] == 0.25:\n",
    "            Y_val[j, 1] = 1\n",
    "        elif targets[j] == 0.5:\n",
    "            Y_val[j, 2] = 1\n",
    "        elif targets[j] == 0.75:\n",
    "            Y_val[j, 3] = 1\n",
    "        elif targets[j] == 1.0:\n",
    "            Y_val[j, 4] = 1\n",
    "        else:\n",
    "            print(\"something went wrong, new class\", targets[j])\n",
    "    return Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useValidationData = True\n",
    "\n",
    "tic = time.time()\n",
    "#training_data = pd.read_csv(\"appendedTrainingData_test12021-01-31_2021-03-21.csv\")\n",
    "training_data = pd.read_csv(\"data/numerai_datasets_04.04.21/numerai_training_data.csv\")\n",
    "feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n",
    "\n",
    "training_data = training_data.drop_duplicates()\n",
    "training_data[feature_cols] = training_data[feature_cols].astype(np.float16)\n",
    "training_data.target        = training_data.target.astype(np.float16)\n",
    "\n",
    "\n",
    "X_train = training_data[feature_cols].to_numpy()\n",
    "Y_train = training_data.target.to_numpy()\n",
    "\n",
    "if useValidationData:\n",
    "    tournament_data = pd.read_csv(\"data/numerai_datasets_04.04.21/numerai_tournament_data.csv\")\n",
    "    tournament_data[feature_cols] = tournament_data[feature_cols].astype(np.float16)\n",
    "    tournament_data.target        = tournament_data.target.astype(np.float16)\n",
    "    \n",
    "    \n",
    "\n",
    "    validation_data = tournament_data.loc[tournament_data.data_type == 'validation']\n",
    "    X_val = validation_data[feature_cols].reset_index().drop(['index'], axis = 1).to_numpy()\n",
    "    #X_pred = tournamend_data.loc[tournament_data.data_type == \"\"]\n",
    "    Y_val = validation_data.target.to_numpy()\n",
    "\n",
    "\n",
    "toc = time.time()\n",
    "print(\"processed the data took \", toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv(\"data/numerai_datasets_04.04.21/numerai_training_data.csv\")\n",
    "tournament_data.to_csv(\"data/numerai_datasets_04.04.21/numerai_tournament_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce complexity of dataset by pca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# This implementation uses Minka (2000) to estimate the dimension of the lower-dimensional subspace\n",
    "pca = PCA(n_components='mle')\n",
    "\n",
    "# # Alternatively, we may specify a desired % of variance to keep\n",
    "# pca = PCA(0.95)\n",
    "\n",
    "pca.fit(X_train)\n",
    "pca_train = pca.transform(X_train)\n",
    "pca_test = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train every era separately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all eras in data set\n",
    "eraArray = ['era1']\n",
    "for era in training_data['era']:\n",
    "    if eraArray[len(eraArray)-1] != era:\n",
    "        eraArray.append(era)\n",
    "np.random.shuffle(eraArray)\n",
    "classes = np.array([0, 0.25, 0.5, 0.75, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useNeuralNet = False\n",
    "useXgBoost   = True\n",
    "\n",
    "Y_val = validation_data.target.to_numpy()\n",
    "if useNeuralNet:\n",
    "    Y_valOneHot = oneHotEncodeData(Y_val)\n",
    "    modelNN     = defineNN(X_train.shape[1])\n",
    "    modelNN.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=max_accuracy)\n",
    "if useXgBoost:\n",
    "    model = XGBClassifier(learning_rate=0.1, n_estimators = 100, max_depth=10, colsample_bytree=0.1)\n",
    "    retrainModel = True\n",
    "\n",
    "#loop over all eras to define the training data\n",
    "counter = 0\n",
    "for era in eraArray:\n",
    "    counter += 1\n",
    "    print(\"era \",counter)\n",
    "    \n",
    "    era_data = training_data.loc[training_data.era == era]\n",
    "    X_trainE = era_data[feature_cols].to_numpy()\n",
    "    Y_trainE = era_data.target.to_numpy()\n",
    "    \n",
    "    #process data for training\n",
    "    \n",
    "    if useNeuralNet:\n",
    "        class_weights = calculate_weights(Y_trainE)\n",
    "        Y_trainE      = oneHotEncodeData(Y_trainE)\n",
    "        history       = modelNN.fit(X_trainE, Y_trainE, epochs = 10, batch_size = 256*128, class_weight=class_weights, validation_data=(X_val, Y_valOneHot))\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #model.save_weights(\"model.h5\")\n",
    "        \n",
    "    if useXgBoost:\n",
    "        \n",
    "        class_weights = class_weight.compute_class_weight('balanced', np.array([0, 0.25, 0.5, 0.75, 1]), Y_trainE)\n",
    "        w_array = np.ones(Y_trainE.shape[0], dtype = 'float')\n",
    "        for i, val in enumerate(Y_trainE):\n",
    "            index = np.where(classes == val)\n",
    "            w_array[i] = class_weights[index]\n",
    "            \n",
    "        evalset = [(X_trainE, Y_trainE), (X_val,Y_val)]\n",
    "        \n",
    "        earlyStopping = 10\n",
    "        \n",
    "        if retrainModel:\n",
    "            model.fit(X_trainE, Y_trainE,  eval_set=evalset, sample_weight=w_array, early_stopping_rounds=earlyStopping)\n",
    "            retrainModel = False\n",
    "        else:\n",
    "            model.fit(X_trainE, Y_trainE,  eval_set=evalset, sample_weight=w_array, early_stopping_rounds=earlyStopping, xgb_model='model_eraWise_xgBoost.model')\n",
    "        model.save_model('model_eraWise_xgBoost.model')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN.save_weights(\"model_eraTrained_32Train_39Val.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_oneHot = modelNN.predict(X_val)\n",
    "\n",
    "pred = np.argmax(y_pred_oneHot, axis = 1) / 4\n",
    "print(y_pred_oneHot, pred)\n",
    "    \n",
    "\n",
    "predictions_df = validation_data[\"id\"].to_frame()\n",
    "predictions_df[\"pred\"] = pred\n",
    "predictions_df.pred.hist(bins = 10)\n",
    "print(len(predictions_df.loc[predictions_df.pred != 0.5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
