{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "\n",
    "import neuralNets\n",
    "\n",
    "def oneHotEncodeData3Classes(targets):\n",
    "    j=0\n",
    "    Y_val = np.zeros((targets.shape[0], 3))\n",
    "    for j in range(targets.shape[0]):\n",
    "        if targets[j] == 0:\n",
    "            Y_val[j, 0] = 1\n",
    "        elif targets[j] == 1:\n",
    "            Y_val[j, 1] = 1\n",
    "        elif targets[j] == 2:\n",
    "            Y_val[j, 2] = 1\n",
    "        else:\n",
    "            print(\"something went wrong, new class\", targets[j])\n",
    "    return Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "#training_data = pd.read_csv(\"appendedTrainingData_test12021-01-31_2021-03-21.csv\")\n",
    "training_data = pd.read_csv(\"data/numerai_datasets_02.05.21/numerai_training_data.csv\")\n",
    "feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n",
    "\n",
    "training_data[feature_cols] = training_data[feature_cols].astype(np.float16)\n",
    "training_data.target        = training_data.target.astype(np.float16)\n",
    "\n",
    "# X_train = training_data[feature_cols].to_numpy()\n",
    "# Y_train = training_data.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(\"data/numerai_datasets_02.05.21/numerai_validation_data.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_biased = training_data[feature_cols] - 0.5\n",
    "# X_val_biased = validation_data[feature_cols] - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier between 0/1 0.25/0.75 and 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3class = training_data[feature_cols]\n",
    "#X_train_3class = X_train_biased\n",
    "Y_train_3class = training_data.target\n",
    "\n",
    "Y_train_3class = Y_train_3class.replace(1, 0)\n",
    "Y_train_3class = Y_train_3class.replace([0.25, 0.75], 1)\n",
    "Y_train_3class = Y_train_3class.replace(0.5, 2)\n",
    "\n",
    "X_train_3class = X_train_3class.to_numpy()\n",
    "Y_train_3class = Y_train_3class.to_numpy()\n",
    "\n",
    "X_train_3class, Y_train_3class = shuffle(X_train_3class, Y_train_3class)\n",
    "\n",
    "X_train_3class, X_test_3class, Y_train_3class, Y_test_3class = train_test_split(X_train_3class, Y_train_3class, test_size = 0.3)\n",
    "\n",
    "X_val_3class = validation_data[feature_cols]\n",
    "#X_val_3class = X_val_biased\n",
    "Y_val_3class = validation_data.target\n",
    "\n",
    "Y_val_3class = Y_val_3class.replace(1, 0)\n",
    "Y_val_3class = Y_val_3class.replace([0.25, 0.75], 1)\n",
    "Y_val_3class = Y_val_3class.replace(0.5, 2)\n",
    "\n",
    "X_val_3class = X_val_3class.to_numpy()\n",
    "Y_val_3class = Y_val_3class.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_3class_oneHot = oneHotEncodeData3Classes(Y_train_3class)\n",
    "Y_test_3class_oneHot  = oneHotEncodeData3Classes(Y_test_3class)\n",
    "Y_val_3class_oneHot   = oneHotEncodeData3Classes(Y_val_3class)\n",
    "\n",
    "test_history = []\n",
    "val_history = []\n",
    "class MyCustomCallback_3class(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        res_eval_1 = self.model.evaluate(X_test_3class, Y_test_3class_oneHot, verbose = 0)\n",
    "        res_eval_2 = self.model.evaluate(X_val_3class, Y_val_3class_oneHot, verbose = 0)\n",
    "        test_history.append(res_eval_1[0])\n",
    "        val_history.append(res_eval_2[0])\n",
    "        print(\"test \",res_eval_1)\n",
    "        print(\"val\", res_eval_2)\n",
    "my_val_callback_3class = MyCustomCallback_3class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1, 2]), Y_train_3class)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "test_history = []\n",
    "val_history = []\n",
    "history = modelNN_3classes.fit(X_train_3class, Y_train_3class_oneHot, epochs = 100, class_weight=class_weights, batch_size = 128*256, callbacks = [my_val_callback_3class])#validation_data=(X_test_3class, Y_test_3class_oneHot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_3class_oneHot = oneHotEncodeData3Classes(Y_val_3class)\n",
    "\n",
    "start_date = date(2021,1, 31)\n",
    "path = \"data/numerai_datasets_\"\n",
    "test_date = start_date\n",
    "test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "epochsPerData = 20\n",
    "epochs = 2\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_date = date(2021,1, 31)\n",
    "    path = \"data/numerai_datasets_\"\n",
    "    test_date = start_date\n",
    "    test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "    delta = timedelta(days=7)\n",
    "    \n",
    "    while os.path.exists(test_path):\n",
    "        path_to_csv = test_path + \"/numerai_training_data.csv\" \n",
    "        tic = time.time()\n",
    "        training_data = pd.read_csv(str(path_to_csv))\n",
    "        toc = time.time()\n",
    "        print(\"loaded data \",toc-tic, \"date = \", test_path)\n",
    "        \n",
    "#         if test_date != date(2021, 4, 11):\n",
    "#             path_to_csv_val = test_path + \"/numerai_validation_data.csv\" \n",
    "#             if os.path.exists(path_to_csv_val):\n",
    "#                 val_data = pd.read_csv(str(path_to_csv_val))\n",
    "#                 val_data_3classes = val_data[training_data.columns]\n",
    "                \n",
    "#                 training_data = pd.concat([training_data, val_data_3classes], axis = 0, ignore_index = True)\n",
    "#         else:\n",
    "#             break\n",
    "\n",
    "\n",
    "        X_train_3class = training_data[feature_cols].reset_index().drop(['index'], axis = 1).to_numpy()\n",
    "        Y_train_3class = training_data.target        \n",
    "        \n",
    "        Y_train_3class = Y_train_3class.replace(1, 0)\n",
    "        Y_train_3class = Y_train_3class.replace([0.25, 0.75], 1)\n",
    "        Y_train_3class = Y_train_3class.replace(0.5, 2)        \n",
    "        \n",
    "        Y_train_3class_oneHot = oneHotEncodeData3Classes(Y_train_3class)\n",
    "        #Y_test_3class_oneHot  = oneHotEncodeData3Classes(Y_test_3class)\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1, 2]), Y_train_3class)\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "    \n",
    "        history = modelNN_3classes.fit(X_train_3class, Y_train_3class_oneHot, epochs = epochsPerData, class_weight=class_weights, batch_size = 256*128, validation_data=(X_val_3class, Y_val_3class_oneHot))\n",
    "\n",
    "        test_date = test_date + delta\n",
    "        test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "#import pdb; pdb.set_trace()\n",
    "#model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_3classes.save_weights(\"model_3class_100train_92val_noValData.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelNN_3classes.predict(X_val_3class)\n",
    "pred = np.argmax(pred, axis = 1) #convert one hot vectors to labels\n",
    "\n",
    "accuracy = np.array(Y_val_3class == pred).astype(float).sum() / X_val_3class.shape[0]\n",
    "print(accuracy)\n",
    "predictions_df = pd.DataFrame(pred)\n",
    "#predictions_df[\"pred\"] = pred\n",
    "predictions_df.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelNN_3classes.predict(X_train_3class)\n",
    "pred = np.argmax(pred, axis = 1) #convert one hot vectors to labels\n",
    "\n",
    "accuracy = np.array(Y_train_3class == pred).astype(float).sum() / X_train_3class.shape[0]\n",
    "print(accuracy)\n",
    "predictions_df = pd.DataFrame(pred)\n",
    "#predictions_df[\"pred\"] = pred\n",
    "predictions_df.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df[\"target\"] = Y_train_3class\n",
    "predictions_df.target.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier between 0.25/0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_025 = validation_data.loc[validation_data.target == 0.25]\n",
    "validation_data_075 = validation_data.loc[validation_data.target == 0.75]\n",
    "validation_data_025075 = pd.concat([validation_data_025, validation_data_075], axis = 0, ignore_index = True)\n",
    "\n",
    "\n",
    "X_val_025075 = validation_data_025075[feature_cols].to_numpy()\n",
    "Y_val_025075 = validation_data_025075.target\n",
    "Y_val_025075 = Y_val_025075.replace(0.25, 0)\n",
    "Y_val_025075 = Y_val_025075.replace(0.75, 1)\n",
    "Y_val_025075 = Y_val_025075.to_numpy()\n",
    "\n",
    "# X_val_025075, Y_val_025075 = shuffle(X_val_025075, Y_val_025075)\n",
    "\n",
    "# training_data_025 = training_data.loc[training_data.target == 0.25]\n",
    "# training_data_075 = training_data.loc[training_data.target == 0.75]\n",
    "# training_data_025075 = pd.concat([training_data_025, training_data_075], axis = 0, ignore_index = True)\n",
    "\n",
    "\n",
    "# X_train_025075 = training_data_025075[feature_cols].to_numpy()\n",
    "# Y_train_025075 = training_data_025075.target\n",
    "# Y_train_025075 = Y_train_025075.replace(0.25, 0)\n",
    "# Y_train_025075 = Y_train_025075.replace(0.75, 1)\n",
    "# Y_train_025075 = Y_train_025075.to_numpy()\n",
    "\n",
    "# X_train_025075, Y_train_025075 = shuffle(X_train_025075, Y_train_025075)\n",
    "\n",
    "# X_train_025075, X_test_025075, Y_train_025075, Y_test_025075 = train_test_split(X_train_025075, Y_train_025075, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2021,1, 31)\n",
    "path = \"data/numerai_datasets_\"\n",
    "test_date = start_date\n",
    "test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "epochsPerData = 25\n",
    "epochs = 1\n",
    "counter = 1\n",
    "epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_date = date(2021,4, 25)\n",
    "    path = \"data/numerai_datasets_\"\n",
    "    test_date = start_date\n",
    "    test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "    while os.path.exists(test_path):\n",
    "        path_to_csv = test_path + \"/numerai_training_data.csv\" \n",
    "        tic = time.time()\n",
    "        training_data = pd.read_csv(str(path_to_csv))\n",
    "        toc = time.time()\n",
    "        print(\"loaded data \",toc-tic, \"date = \", test_path)\n",
    "        \n",
    "        training_data_075 = training_data.loc[training_data.target == 0.75]\n",
    "        training_data_025 = training_data.loc[training_data.target == 0.25]\n",
    "        training_data_025075 = pd.concat([training_data_075, training_data_025], axis = 0, ignore_index = True)\n",
    "\n",
    "#         if test_date != date(2021, 4, 18):\n",
    "#             path_to_csv_val = test_path + \"/numerai_validation_data.csv\" \n",
    "#             if os.path.exists(path_to_csv_val):\n",
    "#                 val_data = pd.read_csv(str(path_to_csv_val))\n",
    "#                 val_data_075 = val_data.loc[val_data.target == 0.75]\n",
    "#                 val_data_025 = val_data.loc[val_data.target == 0.25]\n",
    "#                 val_data_025075 = pd.concat([val_data_075, val_data_025], axis = 0, ignore_index = True)\n",
    "#                 val_data_025075 = val_data_025075[training_data.columns]\n",
    "                \n",
    "#                 training_data_025075 = pd.concat([training_data_025075, val_data_025075], axis = 0, ignore_index = True)\n",
    "\n",
    "        Y_train_025075 = training_data_025075.target\n",
    "        Y_train_025075 = Y_train_025075.replace(0.25, 0)\n",
    "        Y_train_025075 = Y_train_025075.replace(0.75, 1)\n",
    "\n",
    "        training_data_025075[\"target\"] = Y_train_025075\n",
    "\n",
    "        if counter == 0:\n",
    "            appendedTraining_data_025075 = training_data_025075\n",
    "        else:\n",
    "            appendedTraining_data_025075 = pd.concat([training_data_025075, appendedTraining_data_025075], axis = 0, ignore_index = True)\n",
    "        \n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "        \n",
    "                \n",
    "#         Y_train_025075 = Y_train_025075.to_numpy()\n",
    "#         X_train_025075 = training_data_025075[feature_cols].to_numpy()\n",
    "        \n",
    "#         X_train_025075, Y_train_025075 = shuffle(X_train_025075, Y_train_025075)\n",
    "\n",
    "#         class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_025075)\n",
    "#         class_weights = dict(enumerate(class_weights))\n",
    "        \n",
    "#         history = modelNN_025075.fit(X_train_025075, Y_train_025075, epochs = epochsPerData, batch_size = 256*128, class_weight=class_weights, validation_data=(X_val_025075, Y_val_025075))\n",
    "        \n",
    "\n",
    "        test_date = test_date + delta\n",
    "        test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #model.save_weights(\"model.h5\")\n",
    "\n",
    "X_train_025075 = appendedTraining_data_025075[feature_cols].to_numpy()\n",
    "Y_train_025075 = appendedTraining_data_025075.target.to_numpy()\n",
    "X_train_025075, Y_train_025075 = shuffle(X_train_025075, Y_train_025075)\n",
    "        \n",
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_025075)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendedTraining_data_025075.to_csv(\"appendedTrainingData_classes_025_075_till_02_05_21_noValData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_075 = training_data.loc[training_data.target == 0.75]\n",
    "training_data_025 = training_data.loc[training_data.target == 0.25]\n",
    "training_data_025075 = pd.concat([training_data_075, training_data_025], axis = 0, ignore_index = True)\n",
    "\n",
    "Y_train_025075 = training_data_025075.target\n",
    "Y_train_025075 = Y_train_025075.replace(0.25, 0)\n",
    "Y_train_025075 = Y_train_025075.replace(0.75, 1)\n",
    "training_data_025075[\"target\"] = Y_train_025075\n",
    "\n",
    "# X_train_025075 = appendedTraining_data_025075[feature_cols].to_numpy()\n",
    "# Y_train_025075 = appendedTraining_data_025075.target.to_numpy()\n",
    "X_train_025075 = training_data_025075[feature_cols].to_numpy()\n",
    "Y_train_025075 = training_data_025075.target.to_numpy()\n",
    "X_train_025075, Y_train_025075 = shuffle(X_train_025075, Y_train_025075)\n",
    "#X_train_025075, X_test_025075, Y_train_025075, Y_test_025075 = train_test_split(X_train_025075, Y_train_025075, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_025075)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_025075 = neuralNets.defineNN_2classes(X_train_025075.shape[1])\n",
    "optAdam    = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.99, beta_2=0.99999)\n",
    "modelNN_025075.compile(optimizer=optAdam, loss='binary_crossentropy', metrics='accuracy')\n",
    "modelNN_025075.load_weights('model_025075_class_99train_94val_noValData.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = modelNN_025075.fit(X_train_025075, Y_train_025075, epochs = 50, batch_size = 256*128, class_weight=class_weights, validation_data=(X_val_025075, Y_val_025075))#(X_test_025075, Y_test_025075)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_025075.save_weights(\"model_025075_class_100train_92test_noValData.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "#print(history.history)\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_accuracyNP(target, pred):\n",
    "    correctVals = 0\n",
    "    n_examples = target.shape[0]\n",
    "    trueArray = (np.rint(pred) == np.reshape(target, (n_examples,1))).astype(float)\n",
    "    wrongValues = 0\n",
    "    for i in range(len(trueArray)):\n",
    "        if trueArray[i] == 1:\n",
    "            correctVals += 1\n",
    "        elif trueArray[i] == 0:\n",
    "            wrongValues += 1\n",
    "        else:\n",
    "            print(\"weird value \", trueArray[i])\n",
    "#     print(trueArray.shape, pred.shape, target.shape)\n",
    "#     for n in range(len(trueArray)):\n",
    "#         print(pred[n], target[n])\n",
    "    print(\"true\", correctVals,\"wrong\", wrongValues, \"n_examples\", n_examples)\n",
    "    return trueArray.sum()/n_examples    \n",
    "\n",
    "def max_accuracy(target, pred):\n",
    "  \"\"\"Calculates how often the max prediction matches one-hot labels.\"\"\"\n",
    "  retVal = 0\n",
    "  #print(pred.shape[0])\n",
    "  if pred.shape[0] != None:  \n",
    "    num_correct_classified = (tf.math.round(pred) == target)\n",
    "    num = tf.reduce_sum(tf.dtypes.cast(num_correct_classified, tf.int32), axis = -1)\n",
    "    retVal = num / pred.shape[0]\n",
    "    print(tf.math.round(pred))#, num_currect_classified, retVal)\n",
    "  else:\n",
    "    retVal = tf.dtypes.cast(tf.math.round(pred) == target, tf.int32)\n",
    "  print(retVal)\n",
    "  return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelNN_025075.predict(X_val_025075)\n",
    "\n",
    "print(max_accuracyNP(Y_val_025075, pred))\n",
    "\n",
    "predictions_df = pd.DataFrame(pred)\n",
    "#predictions_df[\"pred\"] = pred\n",
    "predictions_df.hist(bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 = training_data.loc[training_data.target == 1]\n",
    "training_data_0 = training_data.loc[training_data.target == 0]\n",
    "training_data_01 = pd.concat([training_data_1, training_data_0], axis = 0, ignore_index = True)\n",
    "\n",
    "X_train_01 = training_data_01[feature_cols].to_numpy()\n",
    "Y_train_01 = training_data_01.target.to_numpy()\n",
    "X_train_01, Y_train_01 = shuffle(X_train_01, Y_train_01)\n",
    "\n",
    "#X_train_01, X_test_01, Y_train_01, Y_test_01 = train_test_split(X_train_01, Y_train_01, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_1 = validation_data.loc[validation_data.target == 1]\n",
    "validation_data_0 = validation_data.loc[validation_data.target == 0]\n",
    "validation_data_01 = pd.concat([validation_data_1, validation_data_0], axis = 0, ignore_index = True)\n",
    "\n",
    "X_val_01 = validation_data_01[feature_cols].to_numpy()\n",
    "Y_val_01 = validation_data_01.target.to_numpy()\n",
    "#X_val_01, Y_val_01 = shuffle(X_val_01, Y_val_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_history = []\n",
    "val_history = []\n",
    "class MyCustomCallback_01(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        res_eval_1 = self.model.evaluate(X_test_01, Y_test_01, verbose = 0)\n",
    "        res_eval_2 = self.model.evaluate(X_val_01, Y_val_01, verbose = 0)\n",
    "        test_history.append(res_eval_1[0])\n",
    "        val_history.append(res_eval_2[0])\n",
    "        print(\"test \",res_eval_1)\n",
    "        print(\"val\", res_eval_2)\n",
    "my_val_callback_01 = MyCustomCallback_01()\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_01)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_orig = date(2021,1, 31)\n",
    "start_date_orig = date(2021,4, 25)\n",
    "path = \"data/numerai_datasets_\"\n",
    "test_date = start_date_orig\n",
    "test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "epochsPerData = 25\n",
    "epochs = 1\n",
    "counter = 1\n",
    "epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start_date = start_date_orig\n",
    "    path = \"data/numerai_datasets_\"\n",
    "    test_date = start_date\n",
    "    test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "    while os.path.exists(test_path):\n",
    "        path_to_csv = test_path + \"/numerai_training_data.csv\" \n",
    "        tic = time.time()\n",
    "        training_data = pd.read_csv(str(path_to_csv))\n",
    "        toc = time.time()\n",
    "        print(\"loaded data \",toc-tic, \"date = \", test_path)\n",
    "\n",
    "        \n",
    "        training_data_1 = training_data.loc[training_data.target == 1]\n",
    "        training_data_0 = training_data.loc[training_data.target == 0]\n",
    "        training_data_01 = pd.concat([training_data_1, training_data_0], axis = 0, ignore_index = True)\n",
    "        \n",
    "#         if test_date != date(2021, 4, 11):\n",
    "#             path_to_csv_val = test_path + \"/numerai_validation_data.csv\" \n",
    "#             if os.path.exists(path_to_csv_val):\n",
    "#                 val_data = pd.read_csv(str(path_to_csv_val))\n",
    "#                 val_data_1 = val_data.loc[val_data.target == 1]\n",
    "#                 val_data_0 = val_data.loc[val_data.target == 0]\n",
    "#                 val_data_01 = pd.concat([val_data_1, val_data_0], axis = 0, ignore_index = True)\n",
    "#                 val_data_01 = val_data_01[training_data.columns]\n",
    "#                 training_data_01 = pd.concat([training_data_01, val_data_01], axis = 0, ignore_index = True)\n",
    "        \n",
    "        #append all training data into one dataframe\n",
    "        if counter == 0:\n",
    "            appendedTraining_data_01 = training_data_01\n",
    "        else:\n",
    "            appendedTraining_data_01 = pd.concat([training_data_01, appendedTraining_data_01], axis = 0, ignore_index = True)\n",
    "        \n",
    "#         X_train_01 = training_data_01[feature_cols].to_numpy()\n",
    "#         Y_train_01 = training_data_01.target.to_numpy()\n",
    "#         X_train_01, Y_train_01 = shuffle(X_train_01, Y_train_01)\n",
    "        \n",
    "#         class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_01)\n",
    "#         class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "    \n",
    "#        history = modelNN_01.fit(X_train_01, Y_train_01, epochs = epochsPerData, batch_size = 256*128*2, class_weight=class_weights, validation_data=(X_test_01, Y_test_01))\n",
    "\n",
    "        test_date = test_date + delta\n",
    "        test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "        counter += 1\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #model.save_weights(\"model.h5\")\n",
    " \n",
    "\n",
    "# train with one appended matrix\n",
    "print(appendedTraining_data_01.shape)\n",
    "X_train_01 = appendedTraining_data_01[feature_cols].to_numpy()\n",
    "Y_train_01 = appendedTraining_data_01.target.to_numpy()\n",
    "X_train_01, Y_train_01 = shuffle(X_train_01, Y_train_01)\n",
    "        \n",
    "class_weights = class_weight.compute_class_weight('balanced', np.array([0, 1]), Y_train_01)\n",
    "class_weights = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_01 = neuralNets.defineNN_2classes(X_train_01.shape[1])\n",
    "optAdam    = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.99)\n",
    "modelNN_01.compile(optimizer=optAdam, loss='binary_crossentropy', metrics='accuracy')\n",
    "modelNN_01.load_weights('model_01class_100train_96val_noValData.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = modelNN_01.fit(X_train_01, Y_train_01, epochs = 100, batch_size = 256*128*2, class_weight=class_weights, validation_data=(X_val_01, Y_val_01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appendedTraining_data_01.to_csv(\"appendedTrainingData_classes_0_1_till_18_04_21_noValData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNN_01.save_weights(\"model_01class_100train_95val_noValData.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
