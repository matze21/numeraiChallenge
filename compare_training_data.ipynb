{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Conv1D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "\n",
    "import neuralNets\n",
    "\n",
    "def oneHotEncodeData3Classes(targets):\n",
    "    j=0\n",
    "    Y_val = np.zeros((targets.shape[0], 3))\n",
    "    for j in range(targets.shape[0]):\n",
    "        if targets[j] == 0:\n",
    "            Y_val[j, 0] = 1\n",
    "        elif targets[j] == 1:\n",
    "            Y_val[j, 1] = 1\n",
    "        elif targets[j] == 2:\n",
    "            Y_val[j, 2] = 1\n",
    "        else:\n",
    "            print(\"something went wrong, new class\", targets[j])\n",
    "    return Y_val\n",
    "\n",
    "labelArray = [0, 0.25, 0.5, 0.75, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check training and validation data = all the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 = pd.read_csv(\"data/numerai_datasets_02.05.21/numerai_training_data.csv\")\n",
    "training_data_2 = pd.read_csv(\"data/numerai_datasets_31.01.21/numerai_training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_0 = training_data_2\n",
    "validation_data_0 = pd.read_csv(\"data/numerai_datasets_31.01.21/numerai_validation_data.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0     = training_data_0.drop(columns = [\"id\", \"era\", \"data_type\"]).to_numpy()#[0:10]\n",
    "val_data0 = validation_data_0.drop(columns = [\"Unnamed: 0\", \"id\", \"era\", \"data_type\"]).to_numpy()\n",
    "\n",
    "start_date = date(2021,3, 28)\n",
    "path = \"data/numerai_datasets_\"\n",
    "test_date = start_date\n",
    "test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "delta = timedelta(days=7)\n",
    "\n",
    "epochsPerData = 1\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #start_date = date(2021,1, 31)\n",
    "    path = \"data/numerai_datasets_\"\n",
    "    test_date = start_date\n",
    "    test_path = path + test_date.strftime(\"%d.%m.%y\")\n",
    "    delta = timedelta(days=7)\n",
    "    \n",
    "    while os.path.exists(test_path):\n",
    "        path_to_csv = test_path + \"/numerai_training_data.csv\" \n",
    "        tic = time.time()\n",
    "        training_data = pd.read_csv(str(path_to_csv))\n",
    "        \n",
    "        path_to_csv_val = test_path + \"/numerai_validation_data.csv\" \n",
    "        validation_data = pd.read_csv(str(path_to_csv_val))\n",
    "        \n",
    "        data_comp       = training_data.drop(columns = [\"id\", \"era\", \"data_type\"]).to_numpy()#[0:10]\n",
    "        unnamed         = validation_data.columns[validation_data.columns.str.startswith('Unna')]\n",
    "        validation_data = validation_data.drop(columns = unnamed)\n",
    "        data_comp_val   = validation_data.drop(columns = [\"id\", \"era\", \"data_type\"]).to_numpy()\n",
    "        toc = time.time()\n",
    "        print(\"loaded data \",toc-tic, \"date = \", test_path)\n",
    "        print(\"max difference train= \", np.max(data0-data_comp), \"max difference val =\", np.max(val_data0-data_comp_val))\n",
    "        \n",
    "        test_date = test_date + delta\n",
    "        test_path = path + test_date.strftime(\"%d.%m.%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eraArray = ['era1']\n",
    "for era in training_data_1['era']:\n",
    "    if eraArray[len(eraArray)-1] != era:\n",
    "        eraArray.append(era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era1_1 = training_data_1.loc[training_data_1.era == \"era1\"]\n",
    "era1_2 = training_data_2.loc[training_data_2.era == \"era1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = training_data_1.drop(columns = [\"id\", \"era\", \"data_type\"]).to_numpy()#[0:10]\n",
    "data2 = training_data_2.drop(columns = [\"id\", \"era\", \"data_type\"]).to_numpy()#[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data1-data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_training_data = pd.concat([training_data_1, training_data_2], axis = 0, ignore_index = True)\n",
    "processed_training_data = concat_training_data.drop(columns = [\"id\", \"era\", \"data_type\"]).drop_duplicates()\n",
    "print(processed_training_data.shape[0], concat_training_data.shape[0], training_data_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eraNearestNeighbor = []\n",
    "\n",
    "for era in eraArray:\n",
    "    tic = time.time()\n",
    "    era1_1 = training_data_1.loc[training_data_1.era == era]\n",
    "    era1_2 = training_data_2.loc[training_data_2.era == era]\n",
    "    data1 = era1_1.drop(columns = [\"id\", \"era\", \"data_type\"]).to_numpy()#[0:10]\n",
    "    data2 = era1_2.drop(columns = [\"id\", \"era\", \"data_type\"]).to_numpy()#[0:10]\n",
    "\n",
    "    nearestNeighbor = np.zeros((data1.shape[0], 2))\n",
    "    if data1.shape[0] != data2.shape[0]:\n",
    "        print(data1.shape, data2.shape, era)\n",
    "    for i in range(data1.shape[0]):\n",
    "    \n",
    "        vector1 = data1[i, :]\n",
    "        absSumVector = np.sum(np.absolute(vector1 - data2), axis = 1)\n",
    "        index = np.argmin(absSumVector)\n",
    "        nearestNeighbor[i,0] = index\n",
    "        nearestNeighbor[i,1] = absSumVector[index]\n",
    "    toc = time.time()   \n",
    "    print(\"max difference\", max(nearestNeighbor[:,1]), \"min similarity\", min(nearestNeighbor[:,1]), \"elapsed time\", toc-tic)\n",
    "    eraNearestNeighbor.append(nearestNeighbor)\n",
    "    \n",
    "#     break\n",
    "# print(((toc-tic) * data1.shape[0])/3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check tournament data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training data is always exactly the same\n",
    "- no duplicates in the training data\n",
    "\n",
    "Tournament Data:\n",
    "\n",
    "- validation data is the same between all tournament datas\n",
    "- no duplicates in validation data\n",
    "\n",
    "- test data seems to be the same between at least 2 tournament datas, BUT different length (=new examples)\n",
    "- test data has duplicates\n",
    "\n",
    "- live data is similar between tournament datas, but NOT the same\n",
    "- one data point is the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNearestNeighborWihtoutTarget(pd_dataFrame1, pd_dataFrame2):\n",
    "    data1 = pd_dataFrame1.drop(columns = [\"id\", \"era\", \"data_type\", \"target\"]).to_numpy()\n",
    "    data2 = pd_dataFrame2.drop(columns = [\"id\", \"era\", \"data_type\", \"target\"]).to_numpy()\n",
    "    return findNearestNeighborNP(data1,data2)\n",
    "    \n",
    "def findNearestNeighborNP(data1, data2):\n",
    "    nearestNeighbor = np.zeros((data1.shape[0], 2))\n",
    "    if data1.shape[0] != data2.shape[0]:\n",
    "        print(data1.shape, data2.shape, era)\n",
    "    for i in range(data1.shape[0]):\n",
    "        vector1 = data1[i, :]\n",
    "        absSumVector = np.sum(np.absolute(vector1 - data2), axis = 1)\n",
    "        index = np.argmin(absSumVector)\n",
    "        nearestNeighbor[i,0] = index\n",
    "        nearestNeighbor[i,1] = absSumVector[index]\n",
    "    return nearestNeighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament_data_1 = pd.read_csv(\"data/numerai_datasets_02.05.21/numerai_tournament_data.csv\")\n",
    "tournament_data_2 = pd.read_csv(\"data/numerai_datasets_25.04.21/numerai_tournament_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeArray = [\"validation\"]\n",
    "counter = 0\n",
    "for datType in tournament_data_1['data_type']:\n",
    "    if typeArray[len(typeArray)-1] != datType:\n",
    "        print(counter)\n",
    "        typeArray.append(datType)\n",
    "    counter += 1\n",
    "print(typeArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1_val  = tournament_data_1.loc[tournament_data_1.data_type == \"validation\"]#.drop(columns = [\"id\", \"era\", \"data_type\"])\n",
    "data_2_val  = tournament_data_2.loc[tournament_data_2.data_type == \"validation\"]#.drop(columns = [\"id\", \"era\", \"data_type\"])\n",
    "\n",
    "data_1_test = tournament_data_1.loc[tournament_data_1.data_type == \"test\"]#.drop(columns = [\"id\", \"era\", \"data_type\"])\n",
    "data_2_test = tournament_data_2.loc[tournament_data_2.data_type == \"test\"]#.drop(columns = [\"id\", \"era\", \"data_type\"])\n",
    "\n",
    "#not the same\n",
    "data_1_live = tournament_data_1.loc[tournament_data_1.data_type == \"live\"]#.drop(columns = [\"id\", \"era\", \"data_type\"])\n",
    "data_2_live = tournament_data_2.loc[tournament_data_2.data_type == \"live\"]#.drop(columns = [\"id\", \"era\", \"data_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data_1_val.drop(columns = [\"id\", \"era\", \"data_type\"]).to_numpy() - data_2_val.drop(columns = [\"id\", \"era\", \"data_type\"]).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1_test_processed = data_1_test.drop(columns = [\"id\", \"era\", \"data_type\", \"target\"]).drop_duplicates()\n",
    "print(data_1_test_processed.shape, data_1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2_test_processed = data_2_test.drop(columns = [\"id\", \"era\", \"data_type\", \"target\"]).drop_duplicates()\n",
    "print(data_2_test_processed.shape, data_2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(data_1_test_processed.to_numpy()[0:10,:],data_2_test_processed.to_numpy()[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1560702\n",
    "max_test = np.max(data_1_test.drop(columns = [\"id\", \"era\", \"data_type\", \"target\"]).to_numpy()[0:index,:] - data_2_test.drop(columns = [\"id\", \"era\", \"data_type\", \"target\"]).to_numpy()[0:index,:])\n",
    "print(max_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearestNeighbor = findNearestNeighborWihtoutTarget(data_1_live, data_2_live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(nearestNeighbor[:,0], nearestNeighbor[:,1], \"x\")\n",
    "print(nearestNeighbor, np.min(nearestNeighbor[:,1]))\n",
    "counter = 0\n",
    "for i in nearestNeighbor[:,1]:\n",
    "    if i == 0:\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_without_target = data1[:,0:data1.shape[1]-1]\n",
    "print(training_data_without_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_without_target = data1[:,0:data1.shape[1]-2]\n",
    "\n",
    "nearestNeighbor = np.zeros((data1.shape[0], 2))\n",
    "    if data1.shape[0] != data2.shape[0]:\n",
    "        print(data1.shape, data2.shape, era)\n",
    "    for i in range(data1.shape[0]):\n",
    "    \n",
    "        vector1 = data1[i, :]\n",
    "        absSumVector = np.sum(np.absolute(vector1 - data2), axis = 1)\n",
    "        index = np.argmin(absSumVector)\n",
    "        nearestNeighbor[i,0] = index\n",
    "        nearestNeighbor[i,1] = absSumVector[index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
